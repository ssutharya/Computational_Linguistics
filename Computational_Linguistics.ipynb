{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }}
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. write a python program that reads a para and:  \n",
        "    a. tokenizes the text into words;  \n",
        "    b. removes punctuations and converts all words to lowercase;  \n",
        "    c. performs stemming and lemmatization."
      ],
      "metadata": {
        "id": "PXjUrxKijtIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUgK-1eZkQEm",
        "outputId": "c441c70e-ca64-43c0-c6b1-1266cd54b8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hElnteWsjmP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize(para):\n",
        "    # a. tokenizes the text into words;\n",
        "    tokens = nltk.word_tokenize(para)\n",
        "\n",
        "    # b. removes punctuations and converts all words to lowercase;\n",
        "    lowered = [i.lower() for i in tokens if i.isalpha()]\n",
        "\n",
        "    # c. performs stemming and lemmatization.\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(i) for i in lowered]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(i) for i in lowered]\n",
        "\n",
        "    return tokens, lowered, stemmed_tokens, lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e60d725",
        "outputId": "8a69e906-9e4d-4e8a-f7a2-a4d2325df323"
      },
      "source": [
        "paragraph = input(\"Please enter a paragraph: \")\n",
        "\n",
        "tokens, lowered, stemmed_words, lemmatized_words = tokenize(paragraph)\n",
        "\n",
        "print(\"Tokens: \", tokens)\n",
        "print(\"Lower-case after removing punctuations: \", lowered)\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter a paragraph: The cat (Felis catus), also referred to as the domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and working cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Its retractable claws are adapted to killing small prey species such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator.\n",
            "Tokens:  ['The', 'cat', '(', 'Felis', 'catus', ')', ',', 'also', 'referred', 'to', 'as', 'the', 'domestic', 'cat', 'or', 'house', 'cat', ',', 'is', 'a', 'small', 'domesticated', 'carnivorous', 'mammal', '.', 'It', 'is', 'the', 'only', 'domesticated', 'species', 'of', 'the', 'family', 'Felidae', '.', 'Advances', 'in', 'archaeology', 'and', 'genetics', 'have', 'shown', 'that', 'the', 'domestication', 'of', 'the', 'cat', 'occurred', 'in', 'the', 'Near', 'East', 'around', '7500', 'BC', '.', 'It', 'is', 'commonly', 'kept', 'as', 'a', 'pet', 'and', 'working', 'cat', ',', 'but', 'also', 'ranges', 'freely', 'as', 'a', 'feral', 'cat', 'avoiding', 'human', 'contact', '.', 'It', 'is', 'valued', 'by', 'humans', 'for', 'companionship', 'and', 'its', 'ability', 'to', 'kill', 'vermin', '.', 'Its', 'retractable', 'claws', 'are', 'adapted', 'to', 'killing', 'small', 'prey', 'species', 'such', 'as', 'mice', 'and', 'rats', '.', 'It', 'has', 'a', 'strong', ',', 'flexible', 'body', ',', 'quick', 'reflexes', ',', 'and', 'sharp', 'teeth', ',', 'and', 'its', 'night', 'vision', 'and', 'sense', 'of', 'smell', 'are', 'well', 'developed', '.', 'It', 'is', 'a', 'social', 'species', ',', 'but', 'a', 'solitary', 'hunter', 'and', 'a', 'crepuscular', 'predator', '.']\n",
            "Lower-case after removing punctuations:  ['the', 'cat', 'felis', 'catus', 'also', 'referred', 'to', 'as', 'the', 'domestic', 'cat', 'or', 'house', 'cat', 'is', 'a', 'small', 'domesticated', 'carnivorous', 'mammal', 'it', 'is', 'the', 'only', 'domesticated', 'species', 'of', 'the', 'family', 'felidae', 'advances', 'in', 'archaeology', 'and', 'genetics', 'have', 'shown', 'that', 'the', 'domestication', 'of', 'the', 'cat', 'occurred', 'in', 'the', 'near', 'east', 'around', 'bc', 'it', 'is', 'commonly', 'kept', 'as', 'a', 'pet', 'and', 'working', 'cat', 'but', 'also', 'ranges', 'freely', 'as', 'a', 'feral', 'cat', 'avoiding', 'human', 'contact', 'it', 'is', 'valued', 'by', 'humans', 'for', 'companionship', 'and', 'its', 'ability', 'to', 'kill', 'vermin', 'its', 'retractable', 'claws', 'are', 'adapted', 'to', 'killing', 'small', 'prey', 'species', 'such', 'as', 'mice', 'and', 'rats', 'it', 'has', 'a', 'strong', 'flexible', 'body', 'quick', 'reflexes', 'and', 'sharp', 'teeth', 'and', 'its', 'night', 'vision', 'and', 'sense', 'of', 'smell', 'are', 'well', 'developed', 'it', 'is', 'a', 'social', 'species', 'but', 'a', 'solitary', 'hunter', 'and', 'a', 'crepuscular', 'predator']\n",
            "Stemmed words: ['the', 'cat', 'feli', 'catu', 'also', 'refer', 'to', 'as', 'the', 'domest', 'cat', 'or', 'hous', 'cat', 'is', 'a', 'small', 'domest', 'carnivor', 'mammal', 'it', 'is', 'the', 'onli', 'domest', 'speci', 'of', 'the', 'famili', 'felida', 'advanc', 'in', 'archaeolog', 'and', 'genet', 'have', 'shown', 'that', 'the', 'domest', 'of', 'the', 'cat', 'occur', 'in', 'the', 'near', 'east', 'around', 'bc', 'it', 'is', 'commonli', 'kept', 'as', 'a', 'pet', 'and', 'work', 'cat', 'but', 'also', 'rang', 'freeli', 'as', 'a', 'feral', 'cat', 'avoid', 'human', 'contact', 'it', 'is', 'valu', 'by', 'human', 'for', 'companionship', 'and', 'it', 'abil', 'to', 'kill', 'vermin', 'it', 'retract', 'claw', 'are', 'adapt', 'to', 'kill', 'small', 'prey', 'speci', 'such', 'as', 'mice', 'and', 'rat', 'it', 'ha', 'a', 'strong', 'flexibl', 'bodi', 'quick', 'reflex', 'and', 'sharp', 'teeth', 'and', 'it', 'night', 'vision', 'and', 'sens', 'of', 'smell', 'are', 'well', 'develop', 'it', 'is', 'a', 'social', 'speci', 'but', 'a', 'solitari', 'hunter', 'and', 'a', 'crepuscular', 'predat']\n",
            "Lemmatized words: ['the', 'cat', 'felis', 'catus', 'also', 'referred', 'to', 'a', 'the', 'domestic', 'cat', 'or', 'house', 'cat', 'is', 'a', 'small', 'domesticated', 'carnivorous', 'mammal', 'it', 'is', 'the', 'only', 'domesticated', 'specie', 'of', 'the', 'family', 'felidae', 'advance', 'in', 'archaeology', 'and', 'genetics', 'have', 'shown', 'that', 'the', 'domestication', 'of', 'the', 'cat', 'occurred', 'in', 'the', 'near', 'east', 'around', 'bc', 'it', 'is', 'commonly', 'kept', 'a', 'a', 'pet', 'and', 'working', 'cat', 'but', 'also', 'range', 'freely', 'a', 'a', 'feral', 'cat', 'avoiding', 'human', 'contact', 'it', 'is', 'valued', 'by', 'human', 'for', 'companionship', 'and', 'it', 'ability', 'to', 'kill', 'vermin', 'it', 'retractable', 'claw', 'are', 'adapted', 'to', 'killing', 'small', 'prey', 'specie', 'such', 'a', 'mouse', 'and', 'rat', 'it', 'ha', 'a', 'strong', 'flexible', 'body', 'quick', 'reflex', 'and', 'sharp', 'teeth', 'and', 'it', 'night', 'vision', 'and', 'sense', 'of', 'smell', 'are', 'well', 'developed', 'it', 'is', 'a', 'social', 'specie', 'but', 'a', 'solitary', 'hunter', 'and', 'a', 'crepuscular', 'predator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Extract digits, Phone number, and email id from give sentence."
      ],
      "metadata": {
        "id": "mh6YRH2_tTrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_info(text):\n",
        "\n",
        "    # Extract all digits\n",
        "    digits = re.findall(r'\\d', text)\n",
        "\n",
        "    phone_numbers = re.findall(r'\\b(?:\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}|\\d{10,11})\\b', text)\n",
        "\n",
        "    gmail_addresses = re.findall(r'\\b[a-zA-Z0-9._%+-]+@gmail\\.com\\b', text)\n",
        "\n",
        "    return {\n",
        "        \"digits\": digits,\n",
        "        \"phone_numbers\": phone_numbers,\n",
        "        \"gmail_addresses\": gmail_addresses\n",
        "    }\n",
        "\n",
        "text = \"My phone number is 4867473357, or you can call me at 8143586475. My email is sillycat123@gmail.com. Another email is hehethegoat@gmail.com.\"\n",
        "\n",
        "extracted_data = extract_info(text)\n",
        "\n",
        "print(\"Extracted Digits:\", extracted_data[\"digits\"])\n",
        "print(\"Extracted Phone Numbers:\", extracted_data[\"phone_numbers\"])\n",
        "print(\"Extracted Gmail Addresses:\", extracted_data[\"gmail_addresses\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKR-bOfQso0H",
        "outputId": "10fe376f-0b53-41e2-86b0-d44e18b29718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Digits: ['4', '8', '6', '7', '4', '7', '3', '3', '5', '7', '8', '1', '4', '3', '5', '8', '6', '4', '7', '5', '1', '2', '3']\n",
            "Extracted Phone Numbers: ['4867473357', '8143586475']\n",
            "Extracted Gmail Addresses: ['sillycat123@gmail.com', 'hehethegoat@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement simple rule based tokenizer for the English language, using regular expressions. The tokenizer should consider punctuations and special symbols as separate tokens. contractions like 'isn't' should be regarded as two tokens: is and n't, also identify abbreviations(eg: USA) and internal *hyphenation*(ice-cream) as single tokens\n"
      ],
      "metadata": {
        "id": "PAnhy9LHk1Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def rule_based_tokenizer(text):\n",
        "    pattern = re.compile(\n",
        "        r\"([a-zA-Z]+n\\'t)\"                        # contractions like isn't\n",
        "        r\"|([A-Z]{2,})\"                           # abbreviations\n",
        "        r\"|([a-zA-Z]+-[a-zA-Z]+)\"                 # hyphenated words\n",
        "        r\"|(\\$[\\d\\.]+)\"                           # money\n",
        "        r\"|(\\w+)\"                                 # normal words\n",
        "        r\"|(\\S)\"                                  # punctuation, etc.\n",
        "    )\n",
        "\n",
        "    tokens = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        matched_token = next((group for group in match.groups() if group), match.group(0))\n",
        "        tokens.append(matched_token)\n",
        "\n",
        "    # isn't → is, n't\n",
        "    final_tokens = []\n",
        "    contraction_pattern = re.compile(r\"([a-zA-Z]+)(n\\'t)\")\n",
        "\n",
        "    for token in tokens:\n",
        "        match = contraction_pattern.match(token)\n",
        "        if match:\n",
        "            final_tokens.extend([match.group(1), match.group(2)])\n",
        "        else:\n",
        "            final_tokens.append(token)\n",
        "\n",
        "    return final_tokens\n",
        "\n",
        "text = \"won't you tell me how ice-cream is a delicious thing..?\"\n",
        "# text = input(\"Enter Sentence: \")\n",
        "tokens = rule_based_tokenizer(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dTI3jRZrNf-",
        "outputId": "3fb44584-bcd2-453d-90a1-030943c23e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wo', \"n't\", 'you', 'tell', 'me', 'how', 'ice-cream', 'is', 'a', 'delicious', 'thing', '.', '.', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement a text classifier for sentiment analysis using Naive Bayes Theorem"
      ],
      "metadata": {
        "id": "swveBf3QHoo_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdWR-RuNIATG",
        "outputId": "0e399db1-eaa8-4995-cd33-a2f5b6d322da"
      },
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import string\n",
        "import random\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "preprocessed_documents = []\n",
        "for words, category in documents:\n",
        "    # convert to lowercase and remove punctuation\n",
        "    words = [word.lower() for word in words if word not in string.punctuation]\n",
        "\n",
        "    # remove stop words and stem\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "    preprocessed_documents.append((words, category))\n",
        "\n",
        "preprocessed_text = [doc[0] for doc in preprocessed_documents]\n",
        "labels = [doc[1] for doc in preprocessed_documents]\n",
        "\n",
        "print(\"Number of documents:\", len(preprocessed_documents))\n",
        "print(\"Example preprocessed document:\", preprocessed_documents[0])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_text, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Number of samples in training set:\", len(X_train))\n",
        "print(\"Number of samples in testing set:\", len(X_test))\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_str = [' '.join(words) for words in X_train]\n",
        "X_test_str = [' '.join(words) for words in X_test]\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_str)\n",
        "\n",
        "X_test_tfidf = vectorizer.transform(X_test_str)\n",
        "\n",
        "print(\"Shape of X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"Shape of X_test_tfidf:\", X_test_tfidf.shape)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (weighted): {precision:.4f}\")\n",
        "print(f\"Recall (weighted): {recall:.4f}\")\n",
        "print(f\"F1-score (weighted): {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 2000\n",
            "Example preprocessed document: (['sam', 'matthew', 'broderick', 'astronom', 'small', 'american', 'town', 'engag', 'teacher', 'linda', 'kelli', 'preston', 'head', 'heel', 'love', 'linda', 'sudden', 'departur', 'new', 'york', 'citi', 'live', 'new', 'lover', 'anton', 'tch', 'ky', 'karyo', 'come', 'complet', 'surpris', 'know', 'love', 'blind', 'sam', 'leav', 'new', 'york', 'well', 'win', 'back', 'move', 'abandon', 'hous', 'across', 'street', 'anton', 'apart', 'instal', 'camera', 'obscura', 'watch', 'suddenli', 'anton', 'ex', 'maggi', 'show', 'want', 'former', 'lover', 'back', 'contrari', 'want', 'vapor', 'may', 'say', 'want', 'kill', 'possibl', 'bother', 'much', 'either', 'peopl', 'die', 'everi', 'day', 'spite', 'differ', 'motiv', 'two', 'team', 'fall', 'thing', 'move', 'stori', 'sound', 'like', 'someth', 'seen', 'million', 'time', 'griffin', 'dunn', 'bring', 'us', 'charm', 'comedi', 'sinc', 'sleep', 'camera', 'obscura', 'add', 'special', 'someth', 'movi', 'make', 'except', 'romant', 'comedi', 'everyth', 'movi', 'perfect', 'set', 'design', 'cast', 'dialogu', 'inspir', 'touch', 'black', 'humor', 'add', 'certain', 'bite', 'meg', 'ryan', 'known', 'harri', 'met', 'salli', 'sleepless', 'seattl', 'surpris', 'role', 'may', 'final', 'help', 'get', 'rid', 'sweet', 'littl', 'girl', 'imag', 'impress', 'viewer', 'maneat', 'biker', 'girl', 'even', 'surpris', 'fact', 'viewer', 'believ', 'much', 'last', 'year', 'courag', 'fire', 'play', 'gulf', 'war', 'pilot', 'matthew', 'broderick', 'goodnatur', 'somewhat', 'naiv', 'sam', 'troubl', 'accept', 'realiti', 'viewer', 'tell', 'watch', 'star', 'also', 'tri', 'reach', 'troubl', 'keep', 'touch', 'realiti', 'would', 'leav', 'everyth', 'behind', 'chase', 'lost', 'love', 'naivet', 'question', 'minut', 'throughout', 'movi', 'clash', 'maggi', 'exagger', 'wish', 'reveng', 'would', 'anyth', 'harm', 'anton', 'hesit', 'use', 'sam', 'plan', 'make', 'understand', 'right', 'begin', 'noth', 'happen', 'two', 'make', 'scene', 'innoc', 'share', 'bed', 'potato', 'chip', 'watch', 'action', 'across', 'street', 'like', 'saturday', 'night', 'show', 'tv', 'even', 'funnier', 'tch', 'ky', 'karyo', 'perfect', 'anton', 'well', 'repuls', 'say', 'realli', 'manag', 'piti', 'even', 'mishap', 'took', 'sam', 'bride', 'away', 'also', 'french', 'chef', 'world', 'collid', 'e', 'usa', 'franc', 'go', 'wrong', 'even', 'maggi', 'reveng', 'scheme', 'time', 'shock', 'sympath', 'man', 'play', 'je', 'aim', 'moi', 'non', 'plu', 'tape', 'deck', 'everyon', 'captur', 'french', 'charm', 'linda', 'even', 'maggi', 'fell', 'sam', 'certainli', 'bad', 'anton', 'thing', 'take', 'turn', 'wors', 'addict', 'love', 'one', 'year', 'pleasant', 'surpris'], 'pos')\n",
            "Number of samples in training set: 1600\n",
            "Number of samples in testing set: 400\n",
            "Shape of X_train_tfidf: (1600, 24002)\n",
            "Shape of X_test_tfidf: (400, 24002)\n",
            "Accuracy: 0.8075\n",
            "Precision (weighted): 0.8091\n",
            "Recall (weighted): 0.8075\n",
            "F1-score (weighted): 0.8075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing by giving an input sentence and checking if its positive or negative."
      ],
      "metadata": {
        "id": "jNK3bIy6QMSm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ed1173",
        "outputId": "0c8a9670-1fac-4712-e15c-978189afb7f7"
      },
      "source": [
        "def preprocess_input(text):\n",
        "    words = [word.lower() for word in text.split() if word not in string.punctuation]\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "sentence = input(\"Enter a sentence to analyze its sentiment: \")\n",
        "\n",
        "preprocessed_sentence = preprocess_input(sentence)\n",
        "\n",
        "# vectorizing\n",
        "sentence_tfidf = vectorizer.transform([preprocessed_sentence])\n",
        "\n",
        "# predicitng\n",
        "predicted_sentiment = nb_classifier.predict(sentence_tfidf)\n",
        "\n",
        "print(f\"The sentiment of the sentence is: {predicted_sentiment[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to analyze its sentiment: this movie was the last thing i wanted to see\n",
            "The sentiment of the sentence is: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Perform POS Tagging for an Indian lang (hindi since malayalam is too tough) using a pretrained model or dataset.   \n",
        "Task:\n",
        "    - Load a small set of sentences in the target lang.\n",
        "    - Use libraries such as Stanza(?), IndiaNLP, or NLTK to tag parts of speech.\n",
        "    - Identify common tag types and compare with english tags.\n",
        "    - Discuss the challenges of POS tagging in morphologically rich lang.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7a8DVhLDncZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7e057633",
        "outputId": "2a51bbed-56a1-46de-dee7-9bbaeddc520b"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from stanza) (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "24a2a82f925548b99c130e312556e6f5",
            "3f24885acab245baa66ca316aa8728de",
            "07e80ea5a4f64efcbd81f11fbfc8ecfb",
            "390efb2729a249d7838f24621c34acbf",
            "ddda3afa1c254d5fa257f4e09e227622",
            "731e89ad00d3489bbdd40ce745f30bee",
            "ef8bc0dcc44447849a55e991270d0c4f",
            "6ef037771141423cb069ef4b7781efa7",
            "bf4e78277dde4137a3488ee7932365c4",
            "60acdc4902784f7d941ec66e4fecda01",
            "59564573e42a4f978a0af33f90619f69"
          ]
        },
        "id": "80bd0d03",
        "outputId": "08885161-e122-45ea-e4cb-d8f753818434"
      },
      "source": [
        "import stanza\n",
        "\n",
        "stanza.download('hi') # 'hi' is the language code for Hindi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24a2a82f925548b99c130e312556e6f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: hi (Hindi) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/hi/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "01a6bdfedfba4125b5ce7365e10a868f",
            "33885de80bc841f09f79e1beaeaf8fc1",
            "57f1bcbb715e4c168da0927fa7067273",
            "9403b0b529524741992ee35b55d0c75d",
            "d246411692bc44a58b5a383de6a70c13",
            "356f6556bc254b74bdf26a68c892539f",
            "57e43611fcd040529d600f1d2bea31d2",
            "08a2f9a49cde4e9ebb749326bf18d178",
            "27917b943c3543c0b624c50e7a9426c6",
            "2acd940fe6e14e43b0c044c06876e30f",
            "c2dbc6bdc33148dbb5ca5f9327bfc832"
          ]
        },
        "id": "e3f2a7a9",
        "outputId": "722d1369-440c-40e0-c20d-acbf20f66f8a"
      },
      "source": [
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline('hi')\n",
        "\n",
        "hindi_sentences = [\n",
        "    \"मुझे जाते हुए देखो\", # watch me go\n",
        "    \"बिल्ली ने मुझसे रूसी में बात की\" # the cat talked to me in russian\n",
        "]\n",
        "\n",
        "print(\"POS Tagging for Hindi Sentences (using Stanza):\")\n",
        "for sentence in hindi_sentences:\n",
        "    doc = nlp(sentence)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(\"POS Tags:\")\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            print(f\"Word: {word.text}\\tPOS: {word.upos}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01a6bdfedfba4125b5ce7365e10a868f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: hi (Hindi):\n",
            "=============================\n",
            "| Processor | Package       |\n",
            "-----------------------------\n",
            "| tokenize  | hdtb          |\n",
            "| pos       | hdtb_charlm   |\n",
            "| lemma     | hdtb_nocharlm |\n",
            "| depparse  | hdtb_charlm   |\n",
            "| ner       | ilner_charlm  |\n",
            "=============================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: depparse\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging for Hindi Sentences (using Stanza):\n",
            "Sentence: मुझे जाते हुए देखो\n",
            "POS Tags:\n",
            "Word: मुझे\tPOS: PRON\n",
            "Word: जाते\tPOS: VERB\n",
            "Word: हुए\tPOS: AUX\n",
            "Word: देखो\tPOS: VERB\n",
            "--------------------\n",
            "Sentence: बिल्ली ने मुझसे रूसी में बात की\n",
            "POS Tags:\n",
            "Word: बिल्ली\tPOS: NOUN\n",
            "Word: ने\tPOS: ADP\n",
            "Word: मुझसे\tPOS: PRON\n",
            "Word: रूसी\tPOS: NOUN\n",
            "Word: में\tPOS: ADP\n",
            "Word: बात\tPOS: NOUN\n",
            "Word: की\tPOS: VERB\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}